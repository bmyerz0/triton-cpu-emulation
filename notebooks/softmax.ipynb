{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3a6a4a-49cb-4518-b306-7820de38f3cf",
   "metadata": {},
   "source": [
    "# Intro to Triton: softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb4690-148f-467f-ba9f-7ec07def03b3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this example, we will look at softmax, written in Triton. \n",
    "\n",
    "Here is the calculation we will be implementing. For the 2D tensor X, compute softmax on each row. (Row index denoted \"r\" and column index denoted \"c\")\n",
    "\n",
    "For each r, compute:\n",
    "$$rowmax(X_r) = max_c X_r$$\n",
    "$$softmax(X_r) = \\frac{exp(X_r - rowmax(X_r))}{\\sum_c exp(X_r - rowmax(X_r))}$$ \n",
    "\n",
    "## Triton implementation\n",
    "\n",
    "We are going to write a softmax in Triton step-by-step. In each step, we'll have a working Triton program. Each version will introduce a new concept.\n",
    "Here are the steps we'll take:\n",
    "1. Compute softmax on a single row\n",
    "2. Compute softmax on all the rows, with each row computed in parallel\n",
    "3. Generalize so that it works on input whose shape(1) is not limited to powers of two\n",
    "\n",
    "At each step, we will also\n",
    "- demonstrate how to call the Triton kernel from within a Pytorch program\n",
    "- check that we have a correct implementation by comparing the results against those of Pytorch\n",
    "\n",
    "## Version 1: Compute softmax on a single row\n",
    "\n",
    "Here is one way to compute the softmax using Triton, with the caveat that it computes softmax on just the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63dd60d-047c-488d-8b95-7be5290d87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# this annotation marks the function as a Triton program\n",
    "@triton.jit(interpret=True) \n",
    "def softmax_kernel_v1(\n",
    "    output, # pointer to the Pytorch tensor where the result will go \n",
    "    input,  # pointer to the Pytorch tensor containing the input\n",
    "    NUM_COLUMNS: tl.constexpr # number of columns in the input and output tensors; limitation: must be a power of 2\n",
    "):\n",
    "    # form an array of pointers to the elements of the input tensor\n",
    "    col_offsets = tl.arange(0, NUM_COLUMNS)       \n",
    "    input_pointer_array = input + col_offsets\n",
    "\n",
    "    # load the row from the input tensor into a block\n",
    "    row = tl.load(input_pointer_array)\n",
    "\n",
    "    ############################\n",
    "    # compute softmax on the row\n",
    "    # Triton uses similar syntax for operations on blocks\n",
    "    # as is used for numpy arrays and Pytorch tensors\n",
    "    row_minus_max = row - tl.max(row, axis=0)    \n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_of_row = numerator / denominator\n",
    "    ############################\n",
    "\n",
    "    # form an array of pointers to the elements of the output tensor\n",
    "    output_pointer_array = output + col_offsets\n",
    "\n",
    "    # store the result block to the output tensor\n",
    "    tl.store(output_pointer_array, softmax_of_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a214226-cf14-4f52-b167-cdba408d6389",
   "metadata": {},
   "source": [
    "The softmax calculation itself (in the middle) looks similar to how you would write the softmax in Pytorch or Numpy. However, there are some additional details, in the rest of the function: pointers, load, and store.\n",
    "\n",
    "To understand why the additional details are necessary, it is helpful to look at a diagram representing the architecture of a typical GPU. \n",
    "\n",
    "<img src=\"img/architecture_load_store1.png\" alt=\"TODO before practice talk\" width=\"1024\"/>\n",
    "\n",
    "The \"device memory\" is where the Pytorch tensors live. And, there are some number of \"processing units\" that do the computation. We'll focus on a single processing unit for now. In order for the processing unit to do calculations on the data, it must first \"load\" data from device memory into its \"local memory\". And for the result of the calculations to be saved, the processing unit must \"store\" from local memory into device memory.\n",
    "\n",
    "Triton has `tl.load` and `tl.store` for doing load and store. In the above program `row` is a \"block\" of data residing in local memory; a copy of the first row of the input Pytorch tensor.\n",
    "\n",
    "To tell the system what place in device memory we want to load/store to, we need to provide a \"pointer\". A Triton pointer refers to the starting location of some data in device memory (i.e., a location within a Pytorch tensor).\n",
    "A Triton program does not return values directly, rather it stores the result to a Pytorch tensor that has already been created in device memory before the function was called.\n",
    "\n",
    "One more detail. While the pointers passed into the function really just refer to the start of the Pytorch tensor, `tl.load`/`tl.store` actually take an array of pointers. By \"array\" we mean a regular, N-dimensional structure, much like a numpy array or Pytorch tensor. To create this array of pointers, we add the indices of the columns. In this case we want all the columns (indices 0 to `NUM_COLUMNS` - 1). Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572d9f74-aee6-4b04-843f-2ccd602cbf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (tensor) = [[0.549 0.715 0.603 0.545 0.424 0.646]\n",
      " [0.438 0.892 0.964 0.383 0.792 0.529]\n",
      " [0.568 0.926 0.071 0.087 0.02  0.833]\n",
      " [0.778 0.87  0.979 0.799 0.461 0.781]]\n",
      "input (pointer) = 400\n",
      "NUM_COLUMNS = 6\n",
      "col_offsets = [0 1 2 3 4 5]\n",
      "row will be a block of shape (6,), containing a copy of the data at device memory locations [400 401 402 403 404 405]\n",
      "which is the data [0.549 0.715 0.603 0.545 0.424 0.646]\n",
      "\n",
      "input (tensor) = [[0.118 0.64  0.143 0.945 0.522 0.415 0.265 0.774 0.456 0.568]\n",
      " [0.019 0.618 0.612 0.617 0.944 0.682 0.36  0.437 0.698 0.06 ]\n",
      " [0.667 0.671 0.21  0.129 0.315 0.364 0.57  0.439 0.988 0.102]]\n",
      "input (pointer) = 1300\n",
      "NUM_COLUMNS = 10\n",
      "col_offsets = [0 1 2 3 4 5 6 7 8 9]\n",
      "row will be a block of shape (10,), containing a copy of the data at device memory locations [1300 1301 1302 1303 1304 1305 1306 1307 1308 1309]\n",
      "which is the data [0.118 0.64  0.143 0.945 0.522 0.415 0.265 0.774 0.456 0.568]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using numpy to examine your Triton kernel's pointer calculations is a helpful debugging trick!\n",
    "import numpy\n",
    "numpy.random.seed(0)\n",
    "\n",
    "def pointer_example1(input, n_rows, NUM_COLUMNS):\n",
    "    input_data = numpy.round(numpy.random.rand(n_rows, NUM_COLUMNS), 3)\n",
    "    print(\"input (tensor) = {}\".format(input_data))\n",
    "    \n",
    "    # input represents the pointer, which is just a single integer representing the starting location of the input tensor living in device memory\n",
    "    print(\"input (pointer) = {}\".format(input))\n",
    "    print(\"NUM_COLUMNS = {}\".format(NUM_COLUMNS))\n",
    "    \n",
    "    col_offsets = numpy.arange(0, NUM_COLUMNS)\n",
    "    print(\"col_offsets = {}\".format(col_offsets))\n",
    "          \n",
    "    input_pointer_array = input + col_offsets\n",
    "\n",
    "    print(\"row will be a block of shape {}, containing a copy of the data at device memory locations {}\\nwhich is the data {}\\n\".format(input_pointer_array.shape, input_pointer_array, input_data[0][col_offsets]))\n",
    "\n",
    "pointer_example1(400, 4, 6)\n",
    "pointer_example1(1300, 3, 10)\n",
    "# try your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ee7be-e6f2-4d57-a79a-6bb326b8abdf",
   "metadata": {},
   "source": [
    "To ground this in a more familiar analogy, if this was purely numpy/Pytorch code, and `input` was an array/tensor (rather than a pointer), the code would be `row = input[0][0:NUM_COLUMNS]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325e8d8-3bce-484d-a0eb-0c48992010ce",
   "metadata": {},
   "source": [
    "### How to call a Triton kernel\n",
    "\n",
    "Since Triton passes results through pointers, calling a Triton kernel involves an additional detail. We need to allocate a Pytorch tensor where the result will go.\n",
    "\n",
    "To hide this detail (and other details, as we'll see) it is customary to hide the call to a Triton kernel inside a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a534549-c3ad-4c1e-a565-295cb5dc1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Compute softmax of x (limitation: just the first row)\n",
    "def softmax_v1(x):    \n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    n_cols = x.shape[1]\n",
    "\n",
    "    # Call the kernel and wait for it to finish\n",
    "    softmax_kernel_v1[(1,)](y, x, NUM_COLUMNS=n_cols)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ed290-1cc2-493d-9184-25f4c2add9dd",
   "metadata": {},
   "source": [
    "Ignore the `[(1,)]` for now. It has to do with how Triton introduces parallelism, which we'll look at in the next version.\n",
    "\n",
    "Let's test it against Pytorch's built-in softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e45f20-7915-4b31-bec5-a399a165c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
      "        [-0.3160, -2.1152,  0.4681, -0.1577,  1.4437,  0.2660],\n",
      "        [ 0.1665,  0.8744, -0.1435, -0.1116,  0.9318,  1.2590],\n",
      "        [ 2.0050,  0.0537,  0.6181, -0.4128, -0.8411, -2.3160]])\n",
      "torch softmax (row 0)  = tensor([0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121])\n",
      "triton softmax (row 0) = tensor([0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121])\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn((4, 6))\n",
    "print(\"x = {}\".format(x))\n",
    "\n",
    "# test helper\n",
    "def check(expected, actual):\n",
    "    if torch.allclose(expected, actual):\n",
    "        print(\"PASS\")\n",
    "    else:\n",
    "        print(\"FAIL\")\n",
    "\n",
    "# run Pytorch softmax\n",
    "# just look at first row for now\n",
    "torch_softmax_result = torch.softmax(x, dim=1)\n",
    "print(\"torch softmax (row 0)  = {}\".format(torch_softmax_result[0]))\n",
    "\n",
    "# run Triton softmax\n",
    "# just look at first row for now\n",
    "softmax_v1_result = softmax_v1(x)\n",
    "print(\"triton softmax (row 0) = {}\".format(softmax_v1_result[0]))\n",
    "\n",
    "# compare\n",
    "# just look at first row for now\n",
    "check(torch_softmax_result[0], softmax_v1_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895b03dd-7114-4ab3-9eea-9e67f8732d28",
   "metadata": {},
   "source": [
    "### Case study: Using Triton to implement a Pytorch Function\n",
    "\n",
    "To see an example of writing a custom `torch.autograd.Function` in Pytorch using Triton, take a look at the [cross entropy example](https://github.com/openai/triton/blob/04e47d7712e218721e54e741d73729736509abc2/python/triton/ops/cross_entropy.py#L63).\n",
    "In particular to focus on right now:\n",
    "- The `forward` method [calls the Triton kernel](https://github.com/openai/triton/blob/04e47d7712e218721e54e741d73729736509abc2/python/triton/ops/cross_entropy.py#L75) `_forward` (you can name the kernel what you want of course)\n",
    "- The `backward` method [calls the Triton kernel](https://github.com/openai/triton/blob/04e47d7712e218721e54e741d73729736509abc2/python/triton/ops/cross_entropy.py#L93) `_backward`\n",
    "- Reminder that we need to pass the Triton kernel a tensor to put its results in, whether that is [allocated](https://github.com/openai/triton/blob/04e47d7712e218721e54e741d73729736509abc2/python/triton/ops/cross_entropy.py#L72) or [already exists](https://github.com/openai/triton/blob/04e47d7712e218721e54e741d73729736509abc2/python/triton/ops/cross_entropy.py#L88)\n",
    "- While this example implements all the computation in Triton, if you only wanted to implement part of your operator in Triton, you could use a mix of Pytorch and Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dce75e-f782-47e9-bfaf-799e9ef872be",
   "metadata": {},
   "source": [
    "# Version 2: Compute softmax on all the rows, with each row computed in parallel\n",
    "\n",
    "The function `softmax_v1` only computes softmax on the first row of `x`. To compute softmax for all the rows, we can have Triton call the kernel once for each row.\n",
    "The tuple in the `[]` is called the \"launch grid\" and it says how many calls to make. \n",
    "\n",
    "In our existing code, the `[(1,)]` means \"please launch one instance of the kernel.\"\n",
    "\n",
    "```\n",
    "softmax_kernel_v1[(1,)](y, x, NUM_COLUMNS=n_cols)\n",
    "```\n",
    "\n",
    "We can replace the `1` with `x.shape[0]` to say \"please launch `x.shape[0]` (number of rows) instances of the kernel.\"\n",
    "\n",
    "```\n",
    "softmax_kernel_v1[(x.shape[0],)](y, x, NUM_COLUMNS=x.shape[1])\n",
    "```\n",
    "\n",
    "The calls will run in parallel, up to the available processing units on your device.\n",
    "\n",
    "We'll also need to make a modification to the kernel. If we left it as is, it would just compute the first row over and over again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626118ba-6dfb-495b-bb63-8feb9f2d1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# differences from softmax_kernel_v1 are highlighted with comments\n",
    "\n",
    "@triton.jit(interpret=True) \n",
    "def softmax_kernel_v2(\n",
    "    output, \n",
    "    input,  \n",
    "    input_row_stride, # number of elements to get to the next row in input tensor\n",
    "    output_row_stride, # number of elements to get to the next row in output tensor\n",
    "    NUM_COLUMNS: tl.constexpr \n",
    "):\n",
    "    \n",
    "    # find out which row we are assigned\n",
    "    row_index = tl.program_id(0)\n",
    "\n",
    "    # calculate the pointer to the start of our assigned row\n",
    "    row_start_pointer = input + row_index * input_row_stride\n",
    "    \n",
    "    col_offsets = tl.arange(0, NUM_COLUMNS)\n",
    "    # use the row_start_pointer now instead of input (which would always be pointing to row 0)\n",
    "    input_pointer_array = row_start_pointer + col_offsets\n",
    "    \n",
    "    row = tl.load(input_pointer_array)\n",
    "\n",
    "    row_minus_max = row - tl.max(row, axis=0)    \n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    \n",
    "    # calculate the pointer to the start of our assigned row\n",
    "    output_row_start_pointer = output + row_index * output_row_stride\n",
    "    # use the output_row_start_pointer now instead of input (which would always be pointing to row 0)\n",
    "    output_pointer_array = output_row_start_pointer + col_offsets\n",
    "    \n",
    "    tl.store(output_pointer_array, softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdf75d-166e-440b-8c30-eb4e850a77fa",
   "metadata": {},
   "source": [
    "### program_id\n",
    "\n",
    "Triton uses a programming model of Single Program Multiple Data (SPMD). Each call accesses a different subset of the input/output, thereby allowing for parallelism.\n",
    "It is the programmer's job to say which data belongs to each call. We can ask which call by using `tl.program_id(0)`. In the modified kernel, we\n",
    "used `tl.program_id(0)` to pick which row we are on.\n",
    "\n",
    "To seek to the assigned input and output row, we'll need to change the pointer arithmetic to use `row_index`. To do so, we need one more piece of information: stride.\n",
    "\n",
    "### Stride\n",
    "\n",
    "To calculate the pointer to the start of a specific row, we need to take into account how far apart elements are in the row dimension. The computer memory is a 1d array, so addresses are 1d. \n",
    "Along a specific dimension of a tensor, the distance in memory from one element to the next is the \"stride\".\n",
    "\n",
    "`row_start_pointer = input + row_index * input_row_stride`\n",
    "\n",
    "Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bef9168-ccd4-49c4-8154-aa09adf7048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (tensor) = [[0.209 0.161 0.653 0.253 0.466 0.244]\n",
      " [0.159 0.11  0.656 0.138 0.197 0.369]\n",
      " [0.821 0.097 0.838 0.096 0.976 0.469]\n",
      " [0.977 0.605 0.739 0.039 0.283 0.12 ]]\n",
      "input = 400\n",
      "NUM_COLUMNS = 6\n",
      "program_id(0) = 0\n",
      "input_row_stride = 6\n",
      "row_index = 0\n",
      "row_start_pointer = 400\n",
      "col_offsets = [0 1 2 3 4 5]\n",
      "row will be a block of shape (6,), containing a copy of the data at device memory locations [400 401 402 403 404 405]\n",
      "which is the data [0.209 0.161 0.653 0.253 0.466 0.244]\n",
      "\n",
      "input (tensor) = [[0.296 0.119 0.318 0.414 0.064 0.692]\n",
      " [0.567 0.265 0.523 0.094 0.576 0.929]\n",
      " [0.319 0.667 0.132 0.716 0.289 0.183]\n",
      " [0.587 0.02  0.829 0.005 0.678 0.27 ]]\n",
      "input = 400\n",
      "NUM_COLUMNS = 6\n",
      "program_id(0) = 3\n",
      "input_row_stride = 6\n",
      "row_index = 3\n",
      "row_start_pointer = 418\n",
      "col_offsets = [0 1 2 3 4 5]\n",
      "row will be a block of shape (6,), containing a copy of the data at device memory locations [418 419 420 421 422 423]\n",
      "which is the data [0.587 0.02  0.829 0.005 0.678 0.27 ]\n",
      "\n",
      "input (tensor) = [[0.735 0.962 0.249 0.576 0.592 0.572 0.223 0.953 0.447 0.846 0.699 0.297\n",
      "  0.814 0.397 0.881 0.581]\n",
      " [0.882 0.693 0.725 0.501 0.956 0.644 0.424 0.606 0.019 0.302 0.66  0.29\n",
      "  0.618 0.429 0.135 0.298]\n",
      " [0.57  0.591 0.574 0.653 0.652 0.431 0.897 0.368 0.436 0.892 0.806 0.704\n",
      "  0.1   0.919 0.714 0.999]]\n",
      "input = 1300\n",
      "NUM_COLUMNS = 16\n",
      "program_id(0) = 0\n",
      "input_row_stride = 16\n",
      "row_index = 0\n",
      "row_start_pointer = 1300\n",
      "col_offsets = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "row will be a block of shape (16,), containing a copy of the data at device memory locations [1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313\n",
      " 1314 1315]\n",
      "which is the data [0.735 0.962 0.249 0.576 0.592 0.572 0.223 0.953 0.447 0.846 0.699 0.297\n",
      " 0.814 0.397 0.881 0.581]\n",
      "\n",
      "input (tensor) = [[0.149 0.868 0.162 0.616 0.124 0.848 0.807 0.569 0.407 0.069 0.697 0.454\n",
      "  0.722 0.866 0.976 0.856]\n",
      " [0.012 0.36  0.73  0.172 0.521 0.054 0.2   0.019 0.794 0.224 0.345 0.928\n",
      "  0.704 0.032 0.165 0.621]\n",
      " [0.577 0.238 0.934 0.614 0.536 0.59  0.73  0.312 0.398 0.21  0.186 0.944\n",
      "  0.74  0.49  0.227 0.254]]\n",
      "input = 1300\n",
      "NUM_COLUMNS = 16\n",
      "program_id(0) = 1\n",
      "input_row_stride = 16\n",
      "row_index = 1\n",
      "row_start_pointer = 1316\n",
      "col_offsets = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "row will be a block of shape (16,), containing a copy of the data at device memory locations [1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329\n",
      " 1330 1331]\n",
      "which is the data [0.012 0.36  0.73  0.172 0.521 0.054 0.2   0.019 0.794 0.224 0.345 0.928\n",
      " 0.704 0.032 0.165 0.621]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reminder: this is numpy code, used for the purpose of understanding the pointer arithmetic\n",
    "def pointer_example2(input, n_rows, NUM_COLUMNS, program_id0, input_row_stride):\n",
    "    input_data = numpy.round(numpy.random.rand(n_rows, NUM_COLUMNS), 3)\n",
    "    print(\"input (tensor) = {}\".format(input_data))\n",
    "    \n",
    "    print(\"input = {}\".format(input))\n",
    "    print(\"NUM_COLUMNS = {}\".format(NUM_COLUMNS))\n",
    "    print(\"program_id(0) = {}\".format(program_id0))\n",
    "    print(\"input_row_stride = {}\".format(input_row_stride))\n",
    "\n",
    "    row_index = program_id0\n",
    "    print(\"row_index = {}\".format(program_id0))\n",
    "\n",
    "    row_start_pointer = input + row_index * input_row_stride\n",
    "    print(\"row_start_pointer = {}\".format(row_start_pointer))\n",
    "    \n",
    "    col_offsets = numpy.arange(0, NUM_COLUMNS)\n",
    "    print(\"col_offsets = {}\".format(col_offsets))\n",
    "          \n",
    "    input_pointer_array = row_start_pointer + col_offsets\n",
    "\n",
    "    print(\"row will be a block of shape {}, containing a copy of the data at device memory locations {}\\nwhich is the data {}\\n\".format(input_pointer_array.shape, input_pointer_array, input_data[row_index][col_offsets]))\n",
    "\n",
    "pointer_example2(400, 4, 6, 0, 6)\n",
    "pointer_example2(400, 4, 6, 3, 6)\n",
    "pointer_example2(1300, 3, 16, 0, 16)\n",
    "pointer_example2(1300, 3, 16, 1, 16)\n",
    "# try your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c375c-7dfa-4e3d-a68c-defc2cdd3768",
   "metadata": {},
   "source": [
    "### Calling the new kernel\n",
    "\n",
    "Here is how to call the new version of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7152654-f257-4536-866b-9bfcdb15cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_v2_result =\n",
      "tensor([[0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121],\n",
      "        [0.0825, 0.0136, 0.1806, 0.0966, 0.4791, 0.1476],\n",
      "        [0.1036, 0.2103, 0.0760, 0.0785, 0.2227, 0.3089],\n",
      "        [0.6442, 0.0915, 0.1609, 0.0574, 0.0374, 0.0086]])\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "# differences from softmax_v1 are highlighted with comments\n",
    "\n",
    "# Compute softmax of x\n",
    "def softmax_v2(x):    \n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # Call n_rows copies of the kernel\n",
    "    # Triton will make sure each has a unique program_id between 0 and n_rows-1\n",
    "    softmax_kernel_v2[(n_rows,)](y, x,\n",
    "                                 input_row_stride=x.stride(0),  # get the strides from the torch tensors\n",
    "                                 output_row_stride=y.stride(0),\n",
    "                                 NUM_COLUMNS=n_cols)\n",
    "\n",
    "    return y\n",
    "\n",
    "softmax_v2_result = softmax_v2(x)\n",
    "print(\"softmax_v2_result =\\n{}\".format(softmax_v2_result))\n",
    "check(torch_softmax_result, softmax_v2_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a0ab2-999f-4942-b1a6-7168ef89c2f1",
   "metadata": {},
   "source": [
    "#### Launch grid\n",
    "The tuple given within `[(n_rows,)]` is called the \"launch grid\". In this example, we are using a 1d launch grid, `(n_rows,)` which means: call n_rows copies of the kernel, with `tl.program_id(0)` being 0, 1, ..., n_rows-1. \n",
    "\n",
    "Aside: Launch grid is a tuple rather than an integer because Triton also accepts 2d and 3d launch grids, which is convenient for slicing the data in multiple dimensions.\n",
    "\n",
    "#### Strides\n",
    "We also had to pass in arguments for the input and output strides. Pytorch tensors actually have an API for getting the stride of each dimension. In this case we want dim 0, that is, the row dimension.\n",
    "\n",
    "Technicality: It would be good form to also pass in a stride for the column dimension, but we are making an assumption that the data is stored densely in row-major. That is that `stride(0) >= n_cols` and `stride(1) == 1`.\n",
    "\n",
    "### Parallelism\n",
    "\n",
    "Triton gives you different kinds of control over the parallel execution of a kernel.\n",
    "- The programmer specifies the number of instances in the launch grid. That, along with how the kernel uses `program_id`, determines how to parallelize the problem across kernel instances. Each kernel instance MAY be assigned to a different processing unit (\"SM\" on Nvidia GPUs). It is MAY because the number of instances may be greater than the number of available processing units.\n",
    "  - In our example, we each row assigned to a different kernel instance\n",
    "  - Aside: Common term for this parallelism is SPMD\n",
    "- The Triton compiler is in charge of parallelizing operations on blocks onto GPU threads within a single processing unit.\n",
    "  - In our example, all the tensor calculations (max, exp, sum, etc.) are parallelized this way\n",
    "  - Aside: Common terms for this parallelism include SIMD and SIMT\n",
    "- The user can optionally exercise control over scheduling GPU resources with the `num_warps` and `num_stages` parameters (not used in our example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9810e97-3ed3-4f55-b9ff-8d296761e41c",
   "metadata": {},
   "source": [
    "## Version 3: Generalize so that it works on input whose shape(1) is not limited to powers of two\n",
    "\n",
    "Triton actually has a limitation that the loaded/stored block must have dimensions that are a power-of-two. This has to do with the algorithm the GPU compiler currently uses to produce efficient code.\n",
    "\n",
    "**Q: What? But the examples so far all work with `NUM_COLUMNS=6`, which is not a power of two!**\n",
    "- A: The examples work because in this notebook we are running these examples in the Triton interpreter, rather than compiling them.\n",
    " \n",
    "\n",
    "Triton has a concept called \"masks\" for fitting any shape of data into fixed-sized blocks. This concept is illustrated in the following version of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7651cce7-be02-4898-b5f9-c9c51bc2a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit(interpret=True) \n",
    "def softmax_kernel_v3(\n",
    "    output, \n",
    "    input,  \n",
    "    input_row_stride, \n",
    "    output_row_stride,\n",
    "    n_cols,                  # the actual number of columns in input and output\n",
    "    BLOCK_SIZE: tl.constexpr # the desired length of a loaded/stored block\n",
    "):\n",
    "    row_index = tl.program_id(0)\n",
    "\n",
    "    row_start_pointer = input + row_index * input_row_stride\n",
    "    \n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_pointer_array = row_start_pointer + col_offsets\n",
    "\n",
    "    # do not load elements past the end of the last column\n",
    "    # For those remaining elements, set the element in the block to negative infinity\n",
    "    row = tl.load(input_pointer_array, mask=col_offsets < n_cols, other=-float('inf'))\n",
    "\n",
    "    row_minus_max = row - tl.max(row, axis=0)    \n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    \n",
    "    output_row_start_pointer = output + row_index * output_row_stride\n",
    "    output_pointer_array = output_row_start_pointer + col_offsets\n",
    "\n",
    "    # only store those elements from the block that are within bounds of the output tensor\n",
    "    tl.store(output_pointer_array, softmax_output, mask=col_offsets < n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24ae02-cd3d-4085-9f0b-15df9363f495",
   "metadata": {},
   "source": [
    "The mask argument of `tl.load` is a condition over all the elements of the block. For elements where the `mask` is `True`, we'll load from the input tensor. For elements where the `mask` is `False` we fill them with the value given by `other`.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e45eeaf6-9cf5-4a83-afb7-228dc06c63fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (tensor) = [[0.861 0.727 0.27  0.131 0.055 0.302]\n",
      " [0.262 0.456 0.683 0.696 0.284 0.38 ]\n",
      " [0.181 0.789 0.057 0.697 0.779 0.777]\n",
      " [0.259 0.374 0.588 0.273 0.371 0.197]]\n",
      "input = 400\n",
      "n_cols = 6\n",
      "BLOCK_SIZE = 8\n",
      "program_id(0) = 0\n",
      "input_row_stride = 6\n",
      "fill with padding: other = -inf\n",
      "row_index = 0\n",
      "row_start_pointer = 400\n",
      "col_offsets = [0 1 2 3 4 5 6 7]\n",
      "row will be a block of shape (8,), containing a copy of the data at device memory locations [400 401 402 403 404 405 406 407]\n",
      "masked memory locations ['400' '401' '402' '403' '404' '405' 'INV' 'INV']\n",
      "which is the data [0.861 0.727 0.27 0.131 0.055 0.302 -inf -inf]\n",
      "\n",
      "input (tensor) = [[0.46  0.045 0.8   0.077 0.519 0.307]\n",
      " [0.578 0.959 0.646 0.035 0.43  0.51 ]\n",
      " [0.536 0.681 0.278 0.129 0.393 0.956]\n",
      " [0.187 0.904 0.544 0.457 0.882 0.459]]\n",
      "input = 400\n",
      "n_cols = 6\n",
      "BLOCK_SIZE = 8\n",
      "program_id(0) = 3\n",
      "input_row_stride = 6\n",
      "fill with padding: other = -inf\n",
      "row_index = 3\n",
      "row_start_pointer = 418\n",
      "col_offsets = [0 1 2 3 4 5 6 7]\n",
      "row will be a block of shape (8,), containing a copy of the data at device memory locations [418 419 420 421 422 423 424 425]\n",
      "masked memory locations ['418' '419' '420' '421' '422' '423' 'INV' 'INV']\n",
      "which is the data [0.187 0.904 0.544 0.457 0.882 0.459 -inf -inf]\n",
      "\n",
      "input (tensor) = [[0.724 0.399 0.904 0.69  0.7   0.328 0.757 0.636 0.24  0.161 0.796 0.959]\n",
      " [0.458 0.591 0.858 0.457 0.952 0.576 0.821 0.909 0.816 0.159 0.629 0.398]\n",
      " [0.063 0.424 0.259 0.849 0.033 0.959 0.355 0.357 0.016 0.185 0.401 0.929]]\n",
      "input = 1300\n",
      "n_cols = 12\n",
      "BLOCK_SIZE = 16\n",
      "program_id(0) = 0\n",
      "input_row_stride = 16\n",
      "fill with padding: other = 0.0\n",
      "row_index = 0\n",
      "row_start_pointer = 1300\n",
      "col_offsets = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "row will be a block of shape (16,), containing a copy of the data at device memory locations [1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313\n",
      " 1314 1315]\n",
      "masked memory locations ['1300' '1301' '1302' '1303' '1304' '1305' '1306' '1307' '1308' '1309'\n",
      " '1310' '1311' 'INV' 'INV' 'INV' 'INV']\n",
      "which is the data [0.724 0.399 0.904 0.69 0.7 0.328 0.757 0.636 0.24 0.161 0.796 0.959 0.0\n",
      " 0.0 0.0 0.0]\n",
      "\n",
      "input (tensor) = [[0.1   0.945 0.869 0.454 0.327 0.233 0.614 0.033 0.016 0.429 0.068 0.252]\n",
      " [0.221 0.253 0.131 0.012 0.115 0.618 0.974 0.99  0.409 0.163 0.639 0.49 ]\n",
      " [0.989 0.065 0.783 0.288 0.241 0.663 0.246 0.666 0.517 0.424 0.555 0.287]]\n",
      "input = 1300\n",
      "n_cols = 12\n",
      "BLOCK_SIZE = 16\n",
      "program_id(0) = 1\n",
      "input_row_stride = 16\n",
      "fill with padding: other = 0.0\n",
      "row_index = 1\n",
      "row_start_pointer = 1316\n",
      "col_offsets = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "row will be a block of shape (16,), containing a copy of the data at device memory locations [1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329\n",
      " 1330 1331]\n",
      "masked memory locations ['1316' '1317' '1318' '1319' '1320' '1321' '1322' '1323' '1324' '1325'\n",
      " '1326' '1327' 'INV' 'INV' 'INV' 'INV']\n",
      "which is the data [0.221 0.253 0.131 0.012 0.115 0.618 0.974 0.99 0.409 0.163 0.639 0.49 0.0\n",
      " 0.0 0.0 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reminder: this is numpy code, used for the purpose of understanding the pointer arithmetic\n",
    "def pointer_example3(input, n_rows, n_cols, BLOCK_SIZE, program_id0, input_row_stride, other):\n",
    "    input_data = numpy.round(numpy.random.rand(n_rows, n_cols), 3)\n",
    "    print(\"input (tensor) = {}\".format(input_data))\n",
    "    \n",
    "    print(\"input = {}\".format(input))\n",
    "    print(\"n_cols = {}\".format(n_cols))\n",
    "    print(\"BLOCK_SIZE = {}\".format(BLOCK_SIZE))\n",
    "    print(\"program_id(0) = {}\".format(program_id0))\n",
    "    print(\"input_row_stride = {}\".format(input_row_stride))\n",
    "    print(\"fill with padding: other = {}\".format(other))\n",
    "\n",
    "    row_index = program_id0\n",
    "    print(\"row_index = {}\".format(program_id0))\n",
    "\n",
    "    row_start_pointer = input + row_index * input_row_stride\n",
    "    print(\"row_start_pointer = {}\".format(row_start_pointer))\n",
    "    \n",
    "    col_offsets = numpy.arange(0, BLOCK_SIZE)\n",
    "    print(\"col_offsets = {}\".format(col_offsets))\n",
    "          \n",
    "    input_pointer_array = row_start_pointer + col_offsets\n",
    "\n",
    "    # emulating invalid locations in device memory with None\n",
    "    extended_input_row_data = list(input_data[row_index][col_offsets[:n_cols]]) + ([None] * (BLOCK_SIZE - n_cols))\n",
    "    # emulating the padding fill here from the mask=col_offsets < n_cols\n",
    "    data_in_block = numpy.where(col_offsets < n_cols, extended_input_row_data, other)\n",
    "    masked_pointers = numpy.where(col_offsets < n_cols, input_pointer_array, 'INV')\n",
    "                               \n",
    "    print(\"row will be a block of shape {}, containing a copy of the data at device memory locations {}\\nmasked memory locations {}\\nwhich is the data {}\\n\".format(input_pointer_array.shape, input_pointer_array, masked_pointers, data_in_block))\n",
    "\n",
    "pointer_example3(400, 4, 6, 8, 0, 6, -float(\"inf\"))\n",
    "pointer_example3(400, 4, 6, 8, 3, 6, -float(\"inf\"))\n",
    "pointer_example3(1300, 3, 12, 16, 0, 16, 0.0)\n",
    "pointer_example3(1300, 3, 12, 16, 1, 16, 0.0)\n",
    "# try your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abc120-4cf2-48cf-a17e-35092ca128a7",
   "metadata": {},
   "source": [
    "### tl.constexpr\n",
    "\n",
    "Up to this point, we've been ignoring the fact that `BLOCK_SIZE` (previously `NUM_COLUMNS`) is annotated with the type `tl.constexpr`.\n",
    "An expression with type `tl.constexpr`, as in \"constant expression\", means that its value is known at compile time. For example, when\n",
    "`BLOCK_SIZE` is `8`, Triton will compile a version of softmax specifically where `BLOCK_SIZE` is 8. \n",
    "\n",
    "In fact, in Triton all blocks MUST have a shape that is a constant expression. This opens significant opportunities\n",
    "for compiler optimizations.\n",
    "\n",
    "Let's summarize the restrictions on our kernels:\n",
    "- `softmax_kernel_v1` and `softmax_kernel_v2` have the restriction that `NUM_COLUMNS` must be equal to the number of columns in the input (and output).\n",
    "- `softmax_kernel_v3` relaxes the restriction to `n_cols <= BLOCK_SIZE`.\n",
    "\n",
    "Those restrictions come from the way we wrote our Triton code. In general, the only fundamental restrictions are those we mentioned above: shape of blocks must 1) constant expressions and 2) powers of two.\n",
    "(And the power of two restriction is a limitation of the GPU compiler that will eventually be relaxed).\n",
    "\n",
    "### Calling the new kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0bf81b-a474-4f4e-a77a-5573ae176002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_cols = 6; BLOCK_SIZE = 8\n",
      "softmax_v3_result =\n",
      "tensor([[0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121],\n",
      "        [0.0825, 0.0136, 0.1806, 0.0966, 0.4791, 0.1476],\n",
      "        [0.1036, 0.2103, 0.0760, 0.0785, 0.2227, 0.3089],\n",
      "        [0.6442, 0.0915, 0.1609, 0.0574, 0.0374, 0.0086]])\n",
      "PASS\n"
     ]
    }
   ],
   "source": [
    "# differences from softmax_v2 are highlighted with comments\n",
    "\n",
    "def softmax_v3(x):    \n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # For how we've written softmax_kernel_v3, we require n_cols <= BLOCK_SIZE\n",
    "    # Combined with Triton's general restriction that block shape be powers of two,\n",
    "    # we set BLOCK_SIZE to the next power of 2 greater than or equal to n_cols.\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    print(\"n_cols = {}; BLOCK_SIZE = {}\".format(n_cols, BLOCK_SIZE))\n",
    "\n",
    "    softmax_kernel_v3[(n_rows,)](y, x,\n",
    "                                 input_row_stride=x.stride(0),\n",
    "                                 output_row_stride=y.stride(0),\n",
    "                                 n_cols=n_cols, # reminder: kernel needs the number of columns to do the masking\n",
    "                                 BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "    return y\n",
    "\n",
    "softmax_v3_result = softmax_v3(x)\n",
    "print(\"softmax_v3_result =\\n{}\".format(softmax_v3_result))\n",
    "check(torch_softmax_result, softmax_v3_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9a73f-7e2b-4c2d-b48e-9643b7090d2e",
   "metadata": {},
   "source": [
    "### Triton does Just-in-Time compilation (JIT)\n",
    "\n",
    "`BLOCK_SIZE` is a constant expression, yet we are passing the kernel a dynamic argument for that parameter.\n",
    "The reason this works is that launching a Triton kernel actually consists of the Triton runtime system doing two steps: 1) compiling a GPU kernel and 2) running the kernel. \n",
    "So in step (1), the constant expressions are given their specific values. This approach is called JIT.\n",
    "\n",
    "Because step (1) adds latency to running your code, Triton uses a cache so that a kernel need not be re-compiled repeatedly unless either the code or a constexpr is new."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dde8d5-2cd1-4fcb-a2b8-cb9083ba3642",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've taken steps towards being able to do the following:\n",
    "- Write a Triton implementation of a simple calculation\n",
    "- Write code to call the Triton code from Pytorch\n",
    "- Run code with Triton's CPU emulator\n",
    "- Explain many of the key ideas in the Triton language\n",
    "\n",
    "What you might do next:\n",
    "- tweak the examples in this notebook\n",
    "- check out the [Triton tutorials](https://triton-lang.org/main/getting-started/tutorials/index.html) and [documentation](https://triton-lang.org/main/index.html)\n",
    "- run our Triton GPU notebooks in AzureML Studio\n",
    "- [install](https://triton-lang.org/main/getting-started/installation.html) Triton yourself on a GPU-enabled platform\n",
    "\n",
    "## Appendix: CPU Emulator?\n",
    "\n",
    "As of writing this tutorial, Triton [runs on](https://github.com/openai/triton#compatibility) certain Nvidia GPUs. However, there is a useful feature to try programs on any platform that supports Pytorch 2.\n",
    "That's what we refer to as the \"CPU Emulator\". And, it is how we can run Triton code in this notebook without a GPU (e.g., if you are using this within our Github Codespace).\n",
    "\n",
    "Whether or not to use the CPU Emulator or GPU is controlled by this annotation on the Triton kernel:\n",
    "```\n",
    "# Use CPU emulator\n",
    "@triton.jit(interpret=True) \n",
    "\n",
    "# Use GPU\n",
    "@triton.jit\n",
    "```\n",
    "\n",
    "Note: as of writing this tutorial, you need to [build Triton from source](https://triton-lang.org/main/getting-started/installation.html#from-source) to use the CPU Emulator because there is not yet an official release including the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd019731-a86e-4dc9-80a9-c7b02a164885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
