{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3a6a4a-49cb-4518-b306-7820de38f3cf",
   "metadata": {},
   "source": [
    "# Intro to Triton: softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb4690-148f-467f-ba9f-7ec07def03b3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this example, we will look at softmax, written in three different ways:\n",
    "- built-in Pytorch function\n",
    "- Pytorch implementation, optimized by Torch JIT\n",
    "- Triton implementation\n",
    "\n",
    "And, we will compare the performance on a Nvidia V100 GPU.\n",
    "\n",
    "Here is the calculation we will be implementing. For the 2d X, compute softmax on each row. (Row index denoted \"r\" and column index denoted \"c\")\n",
    "\n",
    "For each r, compute:\n",
    "$$rowmax(X_r) = max_c X_r$$\n",
    "$$softmax(X_r) = \\frac{exp(X_r - rowmax(X_r))}{\\sum_c exp(X_r - rowmax(X_r))}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbff496-e3e4-486f-b3c3-3145ece563b7",
   "metadata": {},
   "source": [
    "## Pytorch built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897743a7-4a69-4d4d-b464-6bd9d77fc5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920],\n",
      "        [-0.3160, -2.1152,  0.4681, -0.1577,  1.4437,  0.2660],\n",
      "        [ 0.1665,  0.8744, -0.1435, -0.1116,  0.9318,  1.2590],\n",
      "        [ 2.0050,  0.0537,  0.6181, -0.4128, -0.8411, -2.3160]])\n",
      "tensor([[0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121],\n",
      "        [0.0825, 0.0136, 0.1806, 0.0966, 0.4791, 0.1476],\n",
      "        [0.1036, 0.2103, 0.0760, 0.0785, 0.2227, 0.3089],\n",
      "        [0.6442, 0.0915, 0.1609, 0.0574, 0.0374, 0.0086]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn((4, 6))\n",
    "print(x)\n",
    "\n",
    "torch_softmax_result = torch.softmax(x, dim=1)\n",
    "print(torch_softmax_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75539b78-8609-4a9a-8aa7-40503e5cbc85",
   "metadata": {},
   "source": [
    "## Pytorch implementation, optimized by Torch JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26898f43-b1f6-4884-9733-a9214ac254c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121],\n",
      "        [0.0825, 0.0136, 0.1806, 0.0966, 0.4791, 0.1476],\n",
      "        [0.1036, 0.2103, 0.0760, 0.0785, 0.2227, 0.3089],\n",
      "        [0.6442, 0.0915, 0.1609, 0.0574, 0.0374, 0.0086]])\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def torch_jit_softmax(x):\n",
    "    # for each row, take the max\n",
    "    # shape: R\n",
    "    x_max = x.max(dim=1)[0]\n",
    "\n",
    "    # for each row, subtract the row's max from each element.\n",
    "    # (broadcast x_max to all the columns)\n",
    "    # shape: R x C\n",
    "    z = x - x_max[:, None]\n",
    "\n",
    "    # for each row, complete the numerator\n",
    "    # shape: R x C\n",
    "    numerator = torch.exp(z)\n",
    "    \n",
    "    # for each row, get the sum of the numerator\n",
    "    # shape: R\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    \n",
    "    # for each row, complete the softmax\n",
    "    # (broadcast denominator to all the columns)\n",
    "    ret = numerator / denominator[:, None]\n",
    "    \n",
    "    return ret\n",
    "\n",
    "torch_jit_softmax_result = torch_jit_softmax(x)\n",
    "print(torch_jit_softmax_result)\n",
    "assert torch.allclose(torch_softmax_result, torch_jit_softmax_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48093ba4-1d94-46f2-bf46-ecb68445f7fe",
   "metadata": {},
   "source": [
    "## Triton implementation\n",
    "We are going to write a softmax in Triton step-by-step. In each step, we'll have a working Triton program. Each version will introduce a new concept.\n",
    "Here are the steps we'll take:\n",
    "1. Compute softmax on a single row\n",
    "2. Compute softmax on all the rows, with each row computed in parallel\n",
    "3. Generalize so that it works on input whose shape(1) is not limited to powers of two\n",
    "\n",
    "## Version 1: Compute softmax on a single row\n",
    "\n",
    "Here is one way to compute the softmax using Triton, with the caveat that it computes softmax on just the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63dd60d-047c-488d-8b95-7be5290d87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# this annotation marks the function as a Triton program\n",
    "@triton.jit(interpret=True) \n",
    "def softmax_kernel_v1(\n",
    "    output, # pointer to the Pytorch tensor where the result will go \n",
    "    input,  # pointer to the Pytorch tensor containing the input\n",
    "    NUM_COLUMNS: tl.constexpr # number of columns in the input and output tensors; limitation: must be a power of 2\n",
    "):\n",
    "    # form an array of pointers to the elements of the input tensor\n",
    "    col_offsets = tl.arange(0, NUM_COLUMNS)       \n",
    "    input_pointer_array = input + col_offsets\n",
    "\n",
    "    # load the row from the input tensor into a block\n",
    "    row = tl.load(input_pointer_array)\n",
    "\n",
    "    ############################\n",
    "    # compute softmax on the row\n",
    "    # Triton uses similar syntax for operations on blocks\n",
    "    # as is used for numpy arrays and Pytorch tensors\n",
    "    row_minus_max = row - tl.max(row, axis=0)    \n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_of_row = numerator / denominator\n",
    "    ############################\n",
    "\n",
    "    # form an array of pointers to the elements of the output tensor\n",
    "    output_pointer_array = output + col_offsets\n",
    "\n",
    "    # store the result block to the output tensor\n",
    "    tl.store(output_pointer_array, softmax_of_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a214226-cf14-4f52-b167-cdba408d6389",
   "metadata": {},
   "source": [
    "The softmax calculation itself (in the middle) looks similar to the Pytorch version. However, there are some additional details in the rest of the function: pointers, load, and store.\n",
    "\n",
    "To understand why the additional details are necessary, it is helpful to look at a diagram representing a typical GPU (or CPU or NPU for that matter). \n",
    "\n",
    "<img src=\"img/architecture_load_store1.png\" alt=\"TODO before practice talk\" width=\"1024\"/>\n",
    "\n",
    "The \"device memory\" is where the Pytorch tensors live. And, there are some number of \"processing units\" that do the computation. We'll focus on a single processing unit for now. In order for the processing unit to do calculations on the data, it must first \"load\" data from device memory into its \"local memory\". And for the result of the calculations to be saved, the processing unit must \"store\" from local memory into device memory.\n",
    "\n",
    "Triton has `tl.load` and `tl.store` for doing load and store. In the above program `row` is a \"block\" of data residing in local memory; a copy of the first row of the input Pytorch tensor.\n",
    "\n",
    "To tell the system what place in device memory we want to load/store to, we need to provide a \"pointer\". A Triton pointer refers to the starting location of some data in device memory (i.e., a location within a Pytorch tensor).\n",
    "A Triton program does not return values directly, rather it stores the result to a Pytorch tensor that has already been created in device memory before the function was called.\n",
    "\n",
    "One more detail. While the pointers passed into the function really just refer to the start of the Pytorch tensor, `tl.load`/`tl.store` actually take an array of pointers. By \"array\" we mean a regular, N-dimensional structure, much like a numpy array or Pytorch tensor. To create this array of pointers, we add the indices of the columns. In this case we want all the columns (indices 0 to `NUM_COLUMNS` - 1). Here is an example:\n",
    "\n",
    "- `input: 400` (representing the starting location of the input tensor living in device memory)\n",
    "- `NUM_COLUMNS: 6`\n",
    "- `col_offsets = [0, 1, 2, 3, 4, 5]`\n",
    "- `input_pointer_array = [400, 401, 402, 403, 404, 405]`\n",
    "- `row` will be a 1D block of 6 elements, containing a copy of the data at device memory locations 400, 401, ..., 405\n",
    "\n",
    "To ground this in a more familiar analogy, if this was purely numpy/Pytorch code, and `input` was an array/tensor (rather than a pointer), the code would be `row = input[0][0:NUM_COLUMNS]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325e8d8-3bce-484d-a0eb-0c48992010ce",
   "metadata": {},
   "source": [
    "### How to call a Triton kernel\n",
    "\n",
    "Since Triton passes results through pointers, calling a Triton kernel involves an additional detail. We need to allocate a Pytorch tensor where the result will go.\n",
    "\n",
    "To hide this detail (and other details, as we'll see) it is customary to hide the call to a Triton kernel inside a wrapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a534549-c3ad-4c1e-a565-295cb5dc1044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121])\n"
     ]
    }
   ],
   "source": [
    "# Compute softmax of x (limitation: just the first row)\n",
    "def softmax_v1(x):    \n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    n_cols = x.shape[1]\n",
    "\n",
    "    # Call the kernel and wait for it to finish\n",
    "    softmax_kernel_v1[(1,)](y, x, NUM_COLUMNS=n_cols)\n",
    "\n",
    "    return y\n",
    "\n",
    "softmax_v1_result = softmax_v1(x)\n",
    "print(softmax_v1_result[0])\n",
    "assert torch.allclose(torch_softmax_result[0], softmax_v1_result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af30e7-ac66-4bb9-ac45-a3b817f61030",
   "metadata": {},
   "source": [
    "Ignore the `[(1,)]` for now. It has to do with how Triton introduces parallelism, which we'll look at next!\n",
    "\n",
    "### Taking a step back: what have we gained so far?\n",
    "\n",
    "What we are going to see is that one of the key ingredients that makes Triton programs fast is the ability of the compiler to \"fuse\" sequences of tensor operations together to complete them more efficiently. In general, Pytorch completes each tensor operation one at a time. For example, the max for all the rows is computed before doing any of the subtractions. If the tensor is so large that it doesn't fit in local memory (true for all but the smallest models), this approach is inefficient for the GPU/NPU/CPU. That's because the tensors will be going back and forth between device memory and local memory for every Pytorch operation.\n",
    "\n",
    "This fundamental issue is one that is addressed in Pytorch with features like \"graph mode\" and \"torch.jit.script\". But these optimizers are not always able to produce the most efficient code, particularly for more complex algorithms. In those cases, Triton shines the most. The key is in giving the programmer a bit more control (namely, movement of data between device memory and local memory) to make it easier for Triton to keep data in local memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dce75e-f782-47e9-bfaf-799e9ef872be",
   "metadata": {},
   "source": [
    "# Version 2: Compute softmax on all the rows, with each row computed in parallel\n",
    "\n",
    "The function `softmax_v1` only computes softmax on the first row of `x`. To compute softmax for all the rows, we can have Triton call the kernel once for each row.\n",
    "The tuple in the `[]` is called the \"launch grid\" and it says how many calls to make. We can use the number of rows (`x.shape[0]`) to say once per row.\n",
    "\n",
    "```\n",
    "softmax_kernel_v1[(x.shape[0],)](y, x, NUM_COLUMNS=x.shape[1])\n",
    "```\n",
    "\n",
    "The calls will run in parallel, up to the available processing units on your device.\n",
    "\n",
    "We'll also need to make a modification to the kernel. As is, it would just compute the first row over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626118ba-6dfb-495b-bb63-8feb9f2d1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit(interpret=True) \n",
    "def softmax_kernel_v2(\n",
    "    output, \n",
    "    input,  \n",
    "    input_row_stride, # number of elements to get to the next row in input tensor\n",
    "    output_row_stride, # number of elements to get to the next row in output tensor\n",
    "    NUM_COLUMNS: tl.constexpr \n",
    "):\n",
    "    \n",
    "    # find out which row we are assigned\n",
    "    row_index = tl.program_id(0)\n",
    "\n",
    "    # calculate the pointer to the start of our assigned row\n",
    "    row_start_pointer = input + row_index * input_row_stride\n",
    "    \n",
    "    col_offsets = tl.arange(0, NUM_COLUMNS)\n",
    "    # use the row_start_pointer now instead of input (which would always be pointing to row 0)\n",
    "    input_pointer_array = row_start_pointer + col_offsets\n",
    "    \n",
    "    row = tl.load(input_pointer_array)\n",
    "\n",
    "    row_minus_max = row - tl.max(row, axis=0)    \n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    \n",
    "    # calculate the pointer to the start of our assigned row\n",
    "    output_row_start_pointer = output + row_index * output_row_stride\n",
    "    # use the output_row_start_pointer now instead of input (which would always be pointing to row 0)\n",
    "    output_pointer_array = output_row_start_pointer + col_offsets\n",
    "    \n",
    "    tl.store(output_pointer_array, softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdf75d-166e-440b-8c30-eb4e850a77fa",
   "metadata": {},
   "source": [
    "### program_id\n",
    "\n",
    "Triton uses a programming model of Single Program Multiple Data (SPMD). Each call accesses a different subset of the input/output, thereby allowing for parallelism.\n",
    "It is the programmer's job to say which data belongs to each call. We can ask which call by using `tl.program_id(0)`. In the modified kernel, we\n",
    "used `tl.program_id(0)` to pick which row we are on.\n",
    "\n",
    "To seek to the assigned input and output row, we'll need to change the pointer arithmetic to use `row_index`. To do so, we need one more piece of information: stride.\n",
    "\n",
    "### Stride\n",
    "\n",
    "To calculate the pointer to the start of a specific row, we need to take into account how far apart elements are in the row dimension. The computer memory is a 1d array, so addresses are 1d. \n",
    "Along a specific dimension of a tensor, the distance in memory from one element to the next is the \"stride\".\n",
    "\n",
    "`row_start_pointer = input + row_index * input_row_stride`\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```\n",
    "  tensor([[-0.9247, -0.4253, -2.6438,  0.1452, -0.1209, -0.5797],\n",
    "        [-0.6229, -0.3284, -1.0745, -0.3631, -1.6711,  2.2655],\n",
    "        [ 0.3117, -0.1842,  1.2866,  1.1820, -0.1271,  1.2169],\n",
    "        [ 1.4353,  1.0605, -0.4941, -1.4244, -0.7244, -1.2973]])\n",
    "```\n",
    "\n",
    "* `input: 400` (representing the starting location of the input tensor living in device memory)\n",
    "* `program_id(0): 2` (which says this call is assigned row 2)\n",
    "* `NUM_COLUMNS: 6`\n",
    "* `input_row_stride: 6` (how far to move 1 element along the row dimension)\n",
    "* `row_index = program_id(0) = 2`\n",
    "* `row_start_pointer = 400 + 2 * 6 = 412`  \n",
    "* `col_offsets = [0, 1, 2, 3, 4, 5]`\n",
    "* `input_pointer_array = [412, 413, 414, 415, 416, 417]`\n",
    "* `row` will be a 1D block of 6 elements, containing a copy of the data at device memory locations 412, 413, 414, 415, 416, 417\n",
    "\n",
    "which is `[ 0.3117, -0.1842,  1.2866,  1.1820, -0.1271,  1.2169]`\n",
    "\n",
    "### Calling the new kernel\n",
    "\n",
    "Here is how to call the new version of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7152654-f257-4536-866b-9bfcdb15cfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0507, 0.0494, 0.1216, 0.1012, 0.3650, 0.3121],\n",
      "        [0.0825, 0.0136, 0.1806, 0.0966, 0.4791, 0.1476],\n",
      "        [0.1036, 0.2103, 0.0760, 0.0785, 0.2227, 0.3089],\n",
      "        [0.6442, 0.0915, 0.1609, 0.0574, 0.0374, 0.0086]])\n"
     ]
    }
   ],
   "source": [
    "# Compute softmax of x\n",
    "def softmax_v2(x):    \n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # Call n_rows copies of the kernel\n",
    "    # Triton will make sure each has a unique program_id between 0 and n_rows-1\n",
    "    softmax_kernel_v2[(n_rows,)](y, x,\n",
    "                                 input_row_stride=x.stride(0),\n",
    "                                 output_row_stride=y.stride(0),\n",
    "                                 NUM_COLUMNS=n_cols)\n",
    "\n",
    "    return y\n",
    "\n",
    "softmax_v2_result = softmax_v2(x)\n",
    "print(softmax_v2_result)\n",
    "assert torch.allclose(torch_softmax_result, softmax_v2_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a0ab2-999f-4942-b1a6-7168ef89c2f1",
   "metadata": {},
   "source": [
    "#### Launch grid\n",
    "The tuple given within `[(n_rows,)]` is called the \"launch grid\". In this example, we are using a 1d launch grid, `(n_rows,)` which means: call n_rows copies of the kernel, with `tl.program_id(0)` being 0, 1, ..., n_rows-1. \n",
    "\n",
    "Aside: Launch grid is a tuple rather than an integer because Triton also accepts 2d and 3d launch grids, which is convenient for slicing the data in multiple dimensions.\n",
    "\n",
    "#### Strides\n",
    "We also had to pass in arguments for the input and output strides. Pytorch tensors actually have an API for getting the stride of each dimension. In this case we want dim 0, that is, the row dimension.\n",
    "\n",
    "Technicality: We should be passing in a strides for the column dimension, too, but we are making an assumption that the data is stored row-major. That is that `stride(0) >= n_cols` and `stride(1) == 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9810e97-3ed3-4f55-b9ff-8d296761e41c",
   "metadata": {},
   "source": [
    "## Version 3: Generalize so that it works on input whose shape(1) is not limited to powers of two\n",
    "\n",
    "Triton actually has a limitation that the loaded/stored block must have dimensions that are a power-of-two. This has to do with the algorithm the GPU compiler currently uses to produce efficient code.\n",
    "The examples above still work with of `NUM_COLUMNS=6` because we are running these examples in the Triton interpreter, rather than compiling them.\n",
    "\n",
    "Triton has a concept called \"masks\" for fitting any shape of data into fixed-sized blocks. This concept is illustrated in the following version of softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7651cce7-be02-4898-b5f9-c9c51bc2a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit(interpret=True) \n",
    "def softmax_kernel_v3(\n",
    "    output, \n",
    "    input,  \n",
    "    input_row_stride, \n",
    "    output_row_stride,\n",
    "    n_cols,                  # the actual number of columns in input and output\n",
    "    BLOCK_SIZE: tl.constexpr # the desired length of a loaded/stored block\n",
    "):\n",
    "    row_index = tl.program_id(0)\n",
    "\n",
    "    row_start_pointer = input + row_index * input_row_stride\n",
    "    \n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_pointer_array = row_start_pointer + col_offsets\n",
    "\n",
    "    # do not load elements past the end of the last column\n",
    "    # For those remaining elements, set the element in the block to negative infinity\n",
    "    row = tl.load(input_pointer_array, mask=col_offsets < n_cols, other=-float('inf'))\n",
    "\n",
    "    row_minus_max = row - tl.max(row, axis=0)    \n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    \n",
    "    output_row_start_pointer = output + row_index * output_row_stride\n",
    "    output_pointer_array = output_row_start_pointer + col_offsets\n",
    "\n",
    "    # only store those elements from the block that are within bounds of the output tensor\n",
    "    tl.store(output_pointer_array, softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1d8ea-8501-4491-899c-01980ace7ae9",
   "metadata": {},
   "source": [
    "The mask argument of `tl.load` is a condition over all the elements of the block. For elements where the `mask` is `True`, we'll load from the input tensor. For elements where the `mask` is `False` we fill them with the value given by `other`.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "```\n",
    "  tensor([[-0.9247, -0.4253, -2.6438,  0.1452, -0.1209, -0.5797],\n",
    "        [-0.6229, -0.3284, -1.0745, -0.3631, -1.6711,  2.2655],\n",
    "        [ 0.3117, -0.1842,  1.2866,  1.1820, -0.1271,  1.2169],\n",
    "        [ 1.4353,  1.0605, -0.4941, -1.4244, -0.7244, -1.2973]])\n",
    "```\n",
    "\n",
    "* `input: 400` (representing the starting location of the input tensor living in device memory)\n",
    "* `program_id(0): 2` (which says this call is assigned row 2)\n",
    "* `n_cols: 6`\n",
    "* `BLOCK_SIZE: 8`\n",
    "* `input_row_stride: 6` (how far to move 1 element along the row dimension)\n",
    "* `row_index = program_id(0) = 2`\n",
    "* `row_start_pointer = 400 + 2 * 6 = 418`  \n",
    "* `col_offsets = [0, 1, 2, 3, 4, 5, 6, 7]`\n",
    "* `input_pointer_array = [418, 419, 420, 421, 422, 423, 424, 425]`\n",
    "* `row` will be a 1D block of 8 elements, containing `[0.3117, -0.1842,  1.2866,  1.1820, -0.1271,  1.2169, -inf, -inf]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3be53509-d9cb-4f53-9196-c66b2106cf58",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3209652097.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    more use of padding that is more fundamental\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Boundary conditions\n",
    "more use of padding that is more fundamental\n",
    "\n",
    "    # We actually need to give \"load\" the pointers to ALL the data that will go into the block.\n",
    "    # For softmax, since we are computing 1 row, the \"array of pointers\" will have the same shape as the 1 row.\n",
    "    # Triton uses the same syntax for these arrays of pointers as Pytorch/numpy arrays.\n",
    "    # We want to access all the columns in the row.\n",
    "    \n",
    "    # What is BLOCK_SIZE? The size of a block actually needs to be a constant. This helps the compiler\n",
    "    # in generating efficient code.\n",
    "    # To mark an argument as a constant, we use the type annotation tl.constexpr.\n",
    "    # For this softmax, we will want to make sure BLOCK_SIZE >= n_cols. So let BLOCK_SIZE = 256.\n",
    "    \n",
    "    # Using our example, col_offsets = [0, 1, 2, ..., 255] and\n",
    "    # input_ptrs_array = [1504, 1505, 1506, ..., 1759]\n",
    "\n",
    "# What is BLOCK_SIZE? The size of a block actually needs to be a constant. This helps the compiler\n",
    "    # in generating efficient code.\n",
    "    # To mark an argument as a constant, we use the type annotation tl.constexpr.\n",
    "    # For this softmax, we will want to make sure BLOCK_SIZE >= n_cols. So let BLOCK_SIZE = 256.\n",
    "    \n",
    "    # Using our example, col_offsets = [0, 1, 2, ..., 255] and\n",
    "    # input_ptrs_array = [1504, 1505, 1506, ..., 1759]\n",
    "\n",
    "# Load the row from device memory into a block in local memory.\n",
    "    # Since BLOCK_SIZE may be > n_cols, we need Triton to insert padding into the block.\n",
    "    # \"mask\" is the condition to do bounds checking (avoid reading from invalid memory locations) and \"other\" is what value to use\n",
    "    # as the padding.\n",
    "    # In our example, columns 0 to 239 will have valid data, and 240 to 255 will be filled with -inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9fc36-ed3f-49e1-9dad-1a3b3ef92250",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch_softmax(x))\n",
    "print(torch_jit_softmax(x))\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc191347-8fdd-490e-8bd5-d9895cc48539",
   "metadata": {},
   "source": [
    "<img src=\"https://triton-lang.org/main/_images/sphx_glr_02-fused-softmax_001.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915b6c2-80a3-458b-adf3-b4559afa7c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
