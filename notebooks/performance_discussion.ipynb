{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da59be02-7781-41df-827d-386ee20756f9",
   "metadata": {},
   "source": [
    "# Understanding fusion: What and Why\n",
    "\n",
    "One reason to write code in Triton is to \"fuse\" multiple operations together.\n",
    "In this section, we'll illustrate what fusion is, why it improves speed/efficiency,\n",
    "and show some empirical results.\n",
    "\n",
    "## What does fusion look like?\n",
    "\n",
    "Suppose we would like to compute $y = x*sigmoid(x)$, where the size of $x$ is 512 MB.\n",
    "This fits on device memory but far exceeds local memory. Two ways we can compute it are\n",
    "shown below: \"operation at a time\" and \"fused: block at a time.\" \n",
    "\n",
    "<img src=\"img/x_at_a_time.png\" width=1024 alt=\"A comparison of two execution methods: operation at a time and fused block at a time. Operation at a time computes this with sigmoid over whole tensor, in part of the input at a time and saving the result back. Then it does a multiply over the whole tensor, bringing in part of the input/intermediate result at a time and saving the result back. For fused block at a time it brings in a block of input and completes sigmoid and multiply before saving the result back. Then it repeats for each block of the input\">\n",
    "\n",
    "## Why use fusion?\n",
    "\n",
    "To calculate the efficiency of these two approaches, we can count the number of bytes loaded and stored.\n",
    "\n",
    "* Operation at a time\n",
    "  * sigmoid\n",
    "    * load 512 MB for $x$\n",
    "    * store 512 MB for intermediate result\n",
    "  * multiply\n",
    "    * load 512 MB for $x$, load 512 MB for $sigmoid(x)$\n",
    "    * store 512 MB for final result\n",
    "  * total: **1.5 GB load, 1 GB store**\n",
    "* Fused: block at a time\n",
    "  * $x*sigmoid(x)$\n",
    "    * load 512 MB for $x$\n",
    "    * store 512 MB for $x*sigmoid(x)$\n",
    "  * total: **512 MB load, 512 MB store**\n",
    "\n",
    "By keeping data in local memory until it is no longer needed, the fused version is more efficient. \n",
    "\n",
    "## What do frameworks do?\n",
    "\n",
    "Pytorch eager mode does the \"operation at a time\", while compilers (Pytorch graph mode, torch.script.jit, others)\n",
    "should be able to perform certain improvements like using \"fused: block at a time\". In simple cases, this fusion\n",
    "works automatically. But for more complex cases, it may not, which is when Triton becomes useful.\n",
    "\n",
    "## Experiments\n",
    "\n",
    "To see how this analysis applies to practice, we looked at\n",
    "three qualitatively different workloads:\n",
    "- add: a single elementwise operation\n",
    "- swish: multiple elementwise operations\n",
    "- softmax: elementwise + aggregation operations\n",
    "\n",
    "And we looked at four implementations:\n",
    "- Triton\n",
    "- Torch (manual) - handwritten using Pytorch\n",
    "- Torch (manual) JIT - torch.script.jit optimizes the above\n",
    "- Torch (builtin) - call into Pytorch's library\n",
    "\n",
    "We used a V100 GPU, ran each configuration 100 times with a warmup, and only timed\n",
    "the GPU execution portion. We used large enough inputs to keep the device busy.\n",
    "For Triton, we found a block size that maximizes performance for each workload.\n",
    "\n",
    "### Results\n",
    "\n",
    "The following chart shows normalized throughput (higher is better) for each of the workloads and implementations. \n",
    "\n",
    "<img src=\"img/benchmarks results.png\" width=512 alt=\"bar chart showing normalized throughput (higher is better) on 3 benchmarks. Triton/Torch (manual), Torch (manual) JIT, Torch (builtin) is the order of the following numbers. Add: 1, 0.99; swish: 0.995, 0.401, 0.997, 1. Softmax: 1, .244, .390, .817\"/>\n",
    "\n",
    "Here are the interesting observations:\n",
    "- When there is a single operation (add), there is nothing to fuse so handwritten Pytorch reaches peak speed\n",
    "- When there are multiple elementwise operations (swish), fusion is helpful. Handwritten Pytorch does operator-at-a-time so speed suffers. And, in this simple situation, the JIT is able to fuse to reach peak speed. The builtin version (`torch.nn.SiLU`, presumably backed by a CUDA kernel) also reaches peak speed.\n",
    "- When the mixture of operations is more complex (softmax), the JIT is less successful. It appears to be able to improve the handwritten Pytorch but doesn't reach Triton's speed. The builtin (`torch.softmax`) doesn't reach Triton's speed either, possibly due to its generality.\n",
    "\n",
    "## Understanding block size\n",
    "\n",
    "Let's look at how block size affects speed, using the swish example. We fixed a single input size, and we compiled/ran the Triton program with a variety of block sizes.\n",
    "\n",
    "<img src=\"img/block size swish.png\" width=512 alt=\"line chart showing effect of block size on speed in Triton. For example app vector add. Y-axis normalized throughput, x-axis is block size numbered powers of two from 32 to 32K. For sizes 32 to 128 the throughput is 0.2, 0.4, and about 0.88 respectively. From size 256 to 16384, peak or near peak throughput. At 32768 throughput drops off to about 0.3\"/>\n",
    "\n",
    "What we see is that once we get to a certain block size, throughput peaks. Then at large enough block size, the throughput drops off.\n",
    "- In the climbing regime, two possible causes of low throughput:\n",
    "  - overhead of launching many instances dominates running time\n",
    "  - block size small relative to cacheline so memory transfers are inefficient\n",
    "- In the peak regime, there is plenty of work for each instance to do, and the memory bandwidth of the device is saturated with useful work.\n",
    "- At the dropping regime, two possible causes of low throughput:\n",
    "  - (more likely) the data for calculating a block exceeds local memory capacity, in effect reverting execution back to operator-at-a-time \n",
    "  - (less likely) too few instances, so there is not enough parallel work to keep all processing units of the GPU busy. Less likely because total input was large enough for V100's 80 SMs to be busy: $80 * 2^{15} < 2^{26}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f82c69",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
